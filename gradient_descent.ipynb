{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gradient-descent",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chihab216/Implementation-from-scratch-of-svm-and-gradient-descent/blob/master/gradient_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "936cZkY9vE94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import package\n",
        "import numpy as np\n",
        "from random import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2ogPscsvJup",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "b59cec3a-1e20-4dfd-a1d3-88e9daaa34e8"
      },
      "source": [
        "# Initialization\n",
        "x = np.array([[1,1]])\n",
        "x = np.transpose(x)\n",
        "\n",
        "H = np.array([ [7.0, 5.0], [5.0, 7.0] ])\n",
        "b = np.array([[-3.0, 2.0]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[7. 5.]\n",
            " [5. 7.]] [[1]\n",
            " [1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig-XXbHlrIri",
        "colab_type": "text"
      },
      "source": [
        "We calculate the optimal solution with an other solver to see if the result of the others methods are good\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yLT9j8nvT-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We want to minimize this function f(x) = xt * H * x + bt * x\n",
        "def f(x,H,b):\n",
        "  xt = np.transpose(x)\n",
        "  HX = np.dot(H,x)\n",
        "  bt = np.transpose(b)\n",
        "  return(0.5 * np.dot(xt,HX) + np.dot(b,x) )\n",
        "\n",
        "xopt = np.transpose(np.array([[ 1.29166667, -1.20833333]]))#solution optimal du probleme calculer avec un autre solveur"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSZZ5WoZrgOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-RfQwO7SD8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradF(x,H,b):\n",
        "  return(np.dot(H,x)+np.transpose(b))\n",
        "\n",
        "#descente du gradient avec un pas constant(t) et differentes conditions d'arrets\n",
        "#condition d'arret 1\n",
        "def optim(x0,H,b,eps,t):\n",
        "  while np.linalg.norm(gradF(x0,H,b)) > eps:\n",
        "    deltax = -gradF(x0,H,b)\n",
        "    x0 = x0 + t*deltax\n",
        "  return(x0)\n",
        "\n",
        "#condition d'arret 2\n",
        "def optim2(x0,H,b,eps,t):\n",
        "  deltax = -gradF(x0,H,b)\n",
        "  while np.linalg.norm(t*deltax) > eps :\n",
        "    deltax = -gradF(x0,H,b)\n",
        "    x0 = x0 + t*deltax\n",
        "  return(x0)\n",
        "\n",
        "#condition d'arret 3\n",
        "def optim3(x0,H,b,eps,t):\n",
        "  g=f(x0,H,b)\n",
        "  deltax = -gradF(x0,H,b)\n",
        "  x0 = x0 + t*deltax\n",
        "  while np.linalg.norm(g-f(x0,H,b))>eps:\n",
        "    g=f(x0,H,b)\n",
        "    deltax = -gradF(x0,H,b)\n",
        "    x0 = x0 + t*deltax\n",
        "  return(x0)\n",
        "\n",
        "##descente du gradient avec un pas optimal\n",
        "#calcul du pas par recherche du minimum de f(x0+t*deltax,H,b) par methode itérative \n",
        "def optim4(x0,H,b,eps):\n",
        "  alpha= uniform(0.000000001, 0.4999999999999999)\n",
        "  beta= uniform(0.000000000001, 1)\n",
        "  t=1\n",
        "  n=0\n",
        "  while np.linalg.norm(gradF(x0,H,b))>eps and n<100000:\n",
        "    deltax = -gradF(x0,H,b)\n",
        "    while f(x0+t*deltax,H,b)[0][0] > (f(x0,H,b)+ alpha*t*np.dot(np.transpose(gradF(x0,H,b)),deltax))[0][0]:\n",
        "      t=beta*t\n",
        "    x0 = x0 + t*deltax\n",
        "    n=n+1\n",
        "  return(x0)\n",
        "\n",
        "#calcul du pas par recherche du minimum de f(x0+t*deltax,H,b) par methode exacte\n",
        "def optim5(x0,H,b,eps):\n",
        "  while np.linalg.norm(gradF(x0,H,b))>eps  :\n",
        "    deltax = -gradF(x0,H,b)\n",
        "    HD=np.dot(H,deltax)\n",
        "    deltaxt=np.transpose(deltax)\n",
        "    t=abs((np.dot(np.transpose(gradF(x0,H,b)),deltax))[0][0]/(np.dot(deltaxt,HD))[0][0]) \n",
        "    x0 = x0 + t*deltax \n",
        "  return(x0)\n",
        "\n",
        "##descente du gradient avec un sens de descente calculer par la methode de newton\n",
        "def optim6(x0,H,b,eps):\n",
        "  while np.linalg.norm(gradF(x0,H,b))>eps  :\n",
        "    deltax =-np.dot(np.linalg.inv(H),gradF(x0,H,b))\n",
        "    HD=np.dot(H,deltax)\n",
        "    deltaxt=np.transpose(deltax)\n",
        "    t=abs((np.dot(np.transpose(gradF(x0,H,b)),deltax))[0][0]/(np.dot(deltaxt,HD))[0][0]) \n",
        "    x0 = x0 + t*deltax \n",
        "  return(x0)\n",
        "\n",
        "##descente du gradient avec un sens de descente calculer par la methode stochastique mais ne coverge pas\n",
        "def optim7(x0,H,b,eps):\n",
        "  n = H.shape[1]\n",
        "  deltax= np.random.rand(n,1)\n",
        "  p=0\n",
        "  while np.linalg.norm(gradF(x0,H,b)) > eps and p<10000 :\n",
        "    if np.vdot(gradF(x0,H,b),deltax)>0:\n",
        "      deltax=-deltax\n",
        "    if np.vdot(gradF(x0,H,b),deltax)<0:\n",
        "       deltax=deltax\n",
        "    if np.vdot(gradF(x0,H,b),deltax)==0:\n",
        "      deltax= np.random.rand(n,1)\n",
        "    HD=np.dot(H,deltax)\n",
        "    deltaxt=np.transpose(deltax)\n",
        "    t=abs((np.dot(np.transpose(gradF(x0,H,b)),deltax))[0][0]/(np.dot(deltaxt,HD))[0][0]) \n",
        "    x0 = x0 + t*deltax \n",
        "    p=p+1\n",
        "  return(x0)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arztWDXsv5oX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "221b104c-3fce-48bc-be74-5992d4360f45"
      },
      "source": [
        "print(\"Résultats avec un pas constant alpha =0.01 : \")\n",
        "print(\"condition d'arret 1 :norme du gradient inférieur à un epsilone de l'ordre de 10E-8\")\n",
        "print(optim(x,H,b,0.00000001,0.01))\n",
        "print(\"condition d'arret 2 :norme de xk+1 - xk inférieur à un epsilone de l'ordre de 10E-8\")\n",
        "print(optim2(x,H,b,0.00000001,0.01))\n",
        "print(\"condition d'arret 3 :norme de f(xk+1) - f(xk) inférieur à un epsilone de l'ordre de 10E-8\")\n",
        "print(optim3(x,H,b,0.00000001,0.01))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Résultats avec un pas optimal \")\n",
        "print(\"calcul du pas par recherche du minimum de f(x0+t*deltax,H,b) par methode itérative \")\n",
        "print(optim4(x,H,b,0.00000001))\n",
        "print(\"calcul du pas par recherche du minimum de f(x0+t*deltax,H,b) par methode exacte\")\n",
        "print(optim5(x,H,b,0.00000001))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"descente du gradient avec un sens de descente calculer par la methode de newton\")\n",
        "print(optim6(x,H,b,0.00000001))\n",
        "print(\"descente stochastique mais ne donne pas un bon resultat\")\n",
        "print(optim7(x,H,b,0.00000001))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Résultats avec un pas constant alpha =0.01 : \n",
            "condition d'arret 1 :norme du gradient inférieur à un epsilone de l'ordre de 10E-8\n",
            "[[ 1.29166666]\n",
            " [-1.20833333]]\n",
            "condition d'arret 2 :norme de xk+1 - xk inférieur à un epsilone de l'ordre de 10E-8\n",
            "[[ 1.29166632]\n",
            " [-1.20833299]]\n",
            "condition d'arret 3 :norme de f(xk+1) - f(xk) inférieur à un epsilone de l'ordre de 10E-8\n",
            "[[ 1.29132414]\n",
            " [-1.2079908 ]]\n",
            "\n",
            "\n",
            "Résultats avec un pas optimal \n",
            "calcul du pas par recherche du minimum de f(x0+t*deltax,H,b) par methode itérative \n",
            "[[ 1.29166661]\n",
            " [-1.20833327]]\n",
            "calcul du pas par recherche du minimum de f(x0+t*deltax,H,b) par methode exacte\n",
            "[[ 1.29166667]\n",
            " [-1.20833333]]\n",
            "\n",
            "\n",
            "descente du gradient avec un sens de descente calculer par la methode de newton\n",
            "[[ 1.29166667]\n",
            " [-1.20833333]]\n",
            "descente stochastique mais ne donne pas un bon resultat\n",
            "[[0.15775658]\n",
            " [0.27456726]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}